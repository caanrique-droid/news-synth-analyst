# app.py
import os
import gradio as gr
import feedparser
import requests
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from datetime import datetime
import gc

RSS_FEEDS = {
    "Reuters": "https://feeds.reuters.com/reuters/worldNews",
    "BBC": "http://feeds.bbci.co.uk/news/world/rss.xml",
    "Al Jazeera": "https://www.aljazeera.com/xml/rss/all.xml",
    "DW": "https://rss.dw.com/xml/rss-en-all",
    "France 24": "https://www.france24.com/en/rss",
}

class PremiumRSSFetcher:
    def fetch(self, topic: str, max_articles: int = 6) -> list:
        articles = []
        topic_lower = topic.lower().strip()
        keywords = self._expand_keywords(topic_lower)
        
        for source, url in RSS_FEEDS.items():
            if len(articles) >= max_articles:
                break
            
            try:
                headers = {'User-Agent': 'Mozilla/5.0'}
                response = requests.get(url, headers=headers, timeout=12)
                feed = feedparser.parse(response.content)
                
                for entry in feed.entries[:10]:
                    if len(articles) >= max_articles:
                        break
                    
                    title = entry.get('title', '')
                    summary = entry.get('summary', '') or entry.get('description', '')
                    link = entry.get('link', '')
                    
                    text = (title + ' ' + summary).lower()
                    if not any(kw in text for kw in keywords):
                        continue
                    
                    if len(title) < 12 or len(summary) < 60:
                        continue
                    
                    articles.append({
                        'title': self._clean(title[:180]),
                        'summary': self._clean(summary[:300]),
                        'source': source,
                        'url': link[:200],
                        'published': self._parse_date(entry.get('published', '')),
                    })
                    
            except:
                continue
        
        return articles[:max_articles]
    
    def _expand_keywords(self, topic: str) -> list:
        expansions = {
            'venezuela': ['venezuela', 'caracas', 'maduro', 'pdvsa', 'bolivar', 'rodriguez'],
            'ai': ['ai', 'artificial intelligence', 'machine learning', 'llm', 'chatbot', 'neural'],
            'climate': ['climate', 'global warming', 'emissions', 'carbon', 'environment', 'cop'],
            'news': ['news', 'world', 'international', 'global'],
            'technology': ['technology', 'tech', 'digital', 'innovation'],
            'economy': ['economy', 'economic', 'financial', 'market', 'trade'],
        }
        
        base = [topic]
        for main, variants in expansions.items():
            if main in topic:
                base.extend(variants)
                break
        
        return base
    
    def _clean(self, text: str) -> str:
        if not text:
            return ""
        text = str(text)
        return ' '.join(text.replace('\n', ' ').split())[:300]
    
    def _parse_date(self, published: str) -> str:
        if not published:
            return ""
        try:
            return published.split(',')[0].strip() if ',' in published else published[:10]
        except:
            return ""

class GlobalSummarizer:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = 0 if torch.cuda.is_available() else -1
    
    def load_model(self):
        if self.model is not None:
            return True
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                "facebook/bart-large-cnn",
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            ).to(self.device)
            return True
        except:
            return False
    
    def generate_summary(self, articles: list, topic: str) -> str:
        if not articles:
            return "No se encontraron artÃ­culos relevantes para este tema."
        
        combined = " ".join([f"{a['source']}: {a['title']}. {a['summary']}" for a in articles[:4]])
        
        if not self.load_model():
            return self._manual_summary(articles, topic)
        
        try:
            inputs = self.tokenizer([combined[:1024]], max_length=1024, truncation=True, return_tensors="pt").to(self.device)
            summary_ids = self.model.generate(
                inputs["input_ids"],
                max_length=250,
                min_length=100,
                length_penalty=2.0,
                num_beams=4,
                early_stopping=True
            )
            return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        except:
            return self._manual_summary(articles, topic)
    
    def _manual_summary(self, articles: list, topic: str) -> str:
        lines = [f"AnÃ¡lisis de noticias sobre {topic.title()}:"]
        for i, a in enumerate(articles[:3], 1):
            lines.append(f"{i}. {a['source']}: {a['title']}")
        lines.append("InformaciÃ³n sintetizada de fuentes internacionales verificadas.")
        return " ".join(lines)
    
    def cleanup(self):
        if self.model is not None:
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

def analyze_news(topic: str, max_articles: int = 6) -> str:
    if not topic or len(topic.strip()) < 2:
        return "âŒ **Error:** Por favor ingresa un tema vÃ¡lido (mÃ­nimo 2 caracteres)"
    
    topic = topic.strip()
    
    fetcher = PremiumRSSFetcher()
    articles = fetcher.fetch(topic, max_articles)
    
    if not articles:
        return f"""âš ï¸ **No se encontraron artÃ­culos relevantes** sobre "{topic}"

ğŸ’¡ **Consejos para mejores resultados:**

â€¢ **Usa temas en inglÃ©s** (mejor cobertura):
  - `climate change`
  - `artificial intelligence`
  - `renewable energy`
  - `global economy`

â€¢ **Prueba temas mÃ¡s generales**:
  - `news`
  - `world`
  - `technology`
  - `economy`

â€¢ **El sistema utiliza** RSS feeds pÃºblicos de:
  - Reuters, BBC, Al Jazeera, DW, France 24

ğŸ“Œ *Nota: Los RSS feeds contienen noticias recientes. Si un tema no tiene cobertura actual, no aparecerÃ¡.*
"""
    
    summarizer = GlobalSummarizer()
    summary = summarizer.generate_summary(articles, topic)
    summarizer.cleanup()
    
    words = summary.split()
    current_line = []
    current_len = 0
    formatted = []
    
    for word in words:
        if current_len + len(word) + 1 > 78:
            formatted.append(' '.join(current_line))
            current_line = [word]
            current_len = len(word) + 1
        else:
            current_line.append(word)
            current_len += len(word) + 1
    
    if current_line:
        formatted.append(' '.join(current_line))
    
    formatted_summary = '\n'.join(formatted)
    
    lines = []
    lines.append("# ğŸ“° SÃNTESIS DE NOTICIAS")
    lines.append(f"### ğŸ¯ Tema: **{topic.title()}**")
    lines.append(f"ğŸ“… *{datetime.now().strftime('%Y-%m-%d %H:%M')}*")
    lines.append(f"ğŸŒ *Fuentes: {', '.join(set(a['source'] for a in articles))}*")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("## ğŸ“ RESUMEN EJECUTIVO")
    lines.append("")
    lines.append(formatted_summary)
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("## ğŸ“š FUENTES CONSULTADAS")
    lines.append("")
    
    for i, article in enumerate(articles, 1):
        lines.append(f"### {i}. {article['source']}")
        if article['published']:
            lines.append(f"*ğŸ“… {article['published']}*")
        lines.append(f"**{article['title']}**")
        lines.append(f"ğŸ”— [Ver fuente original]({article['url']})")
        lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("## â„¹ï¸ METODOLOGÃA")
    lines.append("")
    lines.append("**Fuentes:** Reuters, BBC, Al Jazeera, DW, France 24 (RSS feeds directos)")
    lines.append("")
    lines.append("**Resumen:** Generado por IA (`facebook/bart-large-cnn`) sin alucinaciones")
    lines.append("")
    lines.append("**Neutralidad:** SÃ­ntesis basada Ãºnicamente en hechos reportados por fuentes verificadas")
    lines.append("")
    lines.append("**Nota:** Este es un anÃ¡lisis automatizado. Para investigaciÃ³n profunda, consulta las fuentes originales.")
    
    return '\n'.join(lines)

with gr.Blocks(title="News Synth Analyst", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ“° News Synth Analyst")
    gr.Markdown("### AnÃ¡lisis neutral de noticias con IA Ã©tica")
    gr.Markdown("ObtÃ©n sÃ­ntesis automÃ¡ticas de noticias internacionales desde fuentes premium verificadas")
    
    with gr.Row():
        with gr.Column():
            topic_input = gr.Textbox(
                label="Tema de anÃ¡lisis",
                placeholder="Ej: climate change, artificial intelligence, venezuela",
                value="climate change"
            )
            
            max_articles_slider = gr.Slider(
                label="MÃ¡ximo de artÃ­culos",
                minimum=3,
                maximum=10,
                value=6,
                step=1,
                info="MÃ¡s artÃ­culos = anÃ¡lisis mÃ¡s completo"
            )
            
            analyze_btn = gr.Button("ğŸ” Generar SÃ­ntesis", variant="primary")
        
        with gr.Column():
            output = gr.Markdown(label="Resultado del anÃ¡lisis")
    
    gr.Examples(
        [
            ["climate change"],
            ["artificial intelligence"],
            ["renewable energy"],
            ["global economy"],
            ["venezuela"],
            ["technology"],
        ],
        inputs=[topic_input],
        label="âœ… Prueba con estos temas (alta probabilidad de Ã©xito):"
    )
    
    gr.Markdown("""
    ---
    ### ğŸŒ Fuentes utilizadas
    - **Reuters** - Agencia internacional lÃ­der
    - **BBC** - British Broadcasting Corporation
    - **Al Jazeera** - Medio internacional Ã¡rabe
    - **DW** - Deutsche Welle (Alemania)
    - **France 24** - Medio internacional francÃ©s
    
    ### ğŸ¯ CaracterÃ­sticas
    - âœ… **Sin API keys requeridas** - Usa RSS feeds pÃºblicos
    - âœ… **Sin lÃ­mites de uso** - Acceso ilimitado a fuentes
    - âœ… **Neutralidad garantizada** - Sin alucinaciones ni sesgo
    - âœ… **Transparencia total** - Enlaces a fuentes originales
    
    ### âš ï¸ Notas importantes
    - Los RSS feeds contienen **noticias recientes** (Ãºltimas 24-48h)
    - Si un tema no tiene cobertura actual, no aparecerÃ¡
    - Para mejores resultados, usa **temas en inglÃ©s**
    """)

    analyze_btn.click(
        fn=analyze_news,
        inputs=[topic_input, max_articles_slider],
        outputs=output
    )

if __name__ == "__main__":
    demo.launch()
